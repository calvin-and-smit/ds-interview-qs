Answer

Bias refers to an error from an estimator that is too general and does not learn relationships from a data set that would allow it to make better predictions.

Variance refers to error from an estimator being too specific and learning relationships that are specific to the training set but will not generalize to new observations well.

ðŸ‘‰ In short, the bias-variance trade-off is a the trade-off between underfitting and overfitting. As you decrease variance, you tend to increase bias. As you decrease bias, you tend to increase variance.

ðŸ‘‰ Generally speaking, your goal is to create models that minimize the overall error by careful model selection and tuning to ensure sure there is a balance between bias and variance: general enough to make good predictions on new data but specific enough to pick up as much signal as possible.
